(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[547],{9765:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/works",function(){return i(2251)}])},944:function(e,t,i){"use strict";i.d(t,{P4:function(){return d},Et:function(){return c},Pg:function(){return l}});var n=i(5893),r=i(1664),a=i(5675),o=i(8527),s=i(917),d=(i(7294),function(e){var t=e.children,i=e.href,r=e.title,s=e.thumbnail;return(0,n.jsx)(o.xu,{w:"100%",align:"center",children:(0,n.jsxs)(o.fG,{cursor:"pointer",children:[(0,n.jsx)(a.default,{src:s,alt:r,className:"grid-item-thumbnail",placeholder:"blur",loading:"lazy"}),(0,n.jsx)(o.AB,{href:i,target:"_blank",children:(0,n.jsx)(o.xv,{mt:2,children:r})}),(0,n.jsx)(o.xv,{fontSize:14,children:t})]})})}),c=function(e){var t=e.children,i=e.id,s=e.title,d=e.thumbnail;return(0,n.jsx)(o.xu,{w:"100%",align:"center",children:(0,n.jsx)(r.default,{href:"/works/".concat(i),children:(0,n.jsxs)(o.fG,{cursor:"pointer",children:[(0,n.jsx)(a.default,{src:d,alt:s,className:"grid-item-thumbnail",placeholder:"blur"}),(0,n.jsx)(o.AB,{href:"/works/".concat(i),children:(0,n.jsx)(o.xv,{mt:2,fontSize:20,children:s})}),(0,n.jsx)(o.xv,{fontSize:14,children:t})]})})})},l=function(){return(0,n.jsx)(s.xB,{styles:"\n        .grid-item-thumbnail{\n            border-radius: 12px;\n        }\n    "})}},2857:function(e,t,i){"use strict";var n=i(5893),r=i(3319),a=i(9008),o=i(944),s={hidden:{opacity:0,x:0,y:20},enter:{opacity:1,x:0,y:0},exit:{opacity:0,x:0,y:20}};t.Z=function(e){var t=e.children,i=e.title;return(0,n.jsx)(r.E.article,{initial:"hidden",animate:"enter",exit:"exit",variants:s,transition:{duration:.4,type:"easeInOut"},style:{position:"relative"},children:(0,n.jsxs)(n.Fragment,{children:[i&&(0,n.jsx)(a.default,{children:(0,n.jsxs)("title",{children:[i," - Saif Rahman"]})}),t,(0,n.jsx)(o.Pg,{})]})})}},7578:function(e,t,i){"use strict";var n=i(5893),r=i(3319),a=i(6052),o=(0,a.m$)(r.E.div,{shouldForwardProp:function(e){return(0,a.x9)(e)||"transition"===e}});t.Z=function(e){var t=e.children,i=e.delay,r=void 0===i?0:i;return(0,n.jsx)(o,{initial:{y:10,opacity:0},animate:{y:0,opacity:1},transtion:{duration:.8,delay:r},mb:6,children:t})}},2251:function(e,t,i){"use strict";i.r(t),i.d(t,{default:function(){return l}});var n=i(5893),r=i(8527),a=i(2857),o=i(7578),s=i(944),d={src:"/_next/static/media/portfolioimage.61971072.png",height:1200,width:2278,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAECAIAAAA8r+mnAAAAbElEQVR42mMwNDIJDgn29gkUE1UT5FEEMhgYOKVkFBmyswvnzl2xdu3GNev3dvQtUZZ3ExO2FBFTZ+DkMWFg8PP1zJs5d8eM2dusNXJdjXOMlfwYTM2sjI1tGBgshPg8GBgclBV85KUcRAUNAWoPGeEUqzs+AAAAAElFTkSuQmCC"},c={src:"/_next/static/media/objectdet.430d0ac5.png",height:1108,width:1802,blurDataURL:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAAAAABd+vKJAAAALklEQVR42gVAwQ0AIQxi/1V9eLkiGqXWIAfj5zBS0T8uo9TspkLlEnUKO9jn8X1jKSagImGSFwAAAABJRU5ErkJggg=="},l=function(){return(0,n.jsx)(a.Z,{title:"Works",children:(0,n.jsxs)(r.W2,{children:[(0,n.jsx)(r.X6,{as:"h3",fontSize:20,mb:4,children:"Works"}),(0,n.jsxs)(r.MI,{column:[1,1,2],gap:6,children:[(0,n.jsx)(o.Z,{children:(0,n.jsx)(s.Et,{id:"universe",title:"Universe",thumbnail:d,children:"An animated interactive portfolio"})}),(0,n.jsx)(o.Z,{children:(0,n.jsx)(s.Et,{id:"Object_det",title:"Object Detection for visually imparied people",thumbnail:c,children:"We recognize distinct objects and precisely get their information through object de- tection, such as their size, shape, and location. This paper developed a low-cost assistive system of obstacle detection and the surround- ing environment depiction to help blind people using deep learning techniques. We worked using TensorFlow object detection API and SSDLite MobileNetV2 to create the proposed object detection model. The pre-trained SSDLite MobileNetV2 model is trained on the COCO dataset, with almost 328,000 images of 90 different objects. Next, we have used the Google text-to-speech module, PyAudio, playsound, and speech recognition to generate the audio feedback of the detected objects. A Raspberry Pi camera captures real-time video where real- time object detection is done frame by frame with Raspberry Pi 4. The proposed device is integrated into a head cap, which will help vi- sually impaired people to detect obstacles in their path, as it is more efficient than a traditional white cane. Apart from this detection model, we trained a secondary computer vision model and named it the \u201cambiance mode.\u201d In this mode, the last three convolutional lay- ers of SSDLite MobileNetV2 are trained through transfer learning on a weather dataset. The dataset consists of around 500 images from four classes: cloudy, rainy, foggy, and sunrise. In this mode, the pro- posed system will narrate the surrounding scene elaborately, almost like a human describing a landscape or a beautiful sunset to a visu- ally impaired person. The object detection and ambiance description mode\u2019s performances are tested and evaluated in a desktop computer and Raspberry Pi embedded system."})})]})]})})}}},function(e){e.O(0,[774,888,179],(function(){return t=9765,e(e.s=t);var t}));var t=e.O();_N_E=t}]);